{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6771eb99",
   "metadata": {},
   "source": [
    "# Analyze MicroMiner hits on the whole PDB\n",
    "Collects all hits of MicroMiner for the PDB and stores them in a SQLite DB for memory efficient analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46e2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "root_dir = Path('/local/sieg/projekte/microminer_evaluation')\n",
    "sys.path.insert(0, str(root_dir.resolve()))\n",
    "import helper\n",
    "from helper.constants import one_2_three_dict, three_2_one_dict, MM_QUERY_NAME, MM_QUERY_CHAIN, MM_QUERY_AA, MM_QUERY_POS, \\\n",
    "                             MM_HIT_NAME, MM_HIT_CHAIN, MM_HIT_AA, MM_HIT_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5284a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name_monomer = \"/local/sieg/mm_pdb_single_mutations_monomer.db\"\n",
    "csv_monomer = '/local/sieg/pdb_all_monomer.tsv'\n",
    "db_name_ppi = \"/local/sieg/mm_pdb_single_mutations_ppi.db\"\n",
    "csv_ppi = '/local/sieg/pdb_all_ppi.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ae7918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVWriterToDatabase:\n",
    "    \"\"\"Converts a MicroMiner output CSV file to a single table in a database\"\"\"\n",
    "    \n",
    "    RESULT_STATISTICS_FIELDS = OrderedDict([(MM_QUERY_NAME, 'TEXT'),\n",
    "                                            (MM_QUERY_CHAIN, 'TEXT'),\n",
    "                                            (MM_QUERY_AA, 'TEXT'), \n",
    "                                            (MM_QUERY_POS, 'TEXT'),\n",
    "                                            (MM_HIT_NAME, 'TEXT'),\n",
    "                                            (MM_HIT_CHAIN, 'TEXT'),\n",
    "                                            (MM_HIT_AA, 'TEXT'),\n",
    "                                            (MM_HIT_POS, 'TEXT'),\n",
    "                                            ('siteIdentity', 'REAL'),\n",
    "                                            ('siteBackBoneRMSD', 'REAL'),\n",
    "                                            ('siteAllAtomRMSD', 'REAL'),\n",
    "                                            ('nofSiteResidues', 'REAL'),\n",
    "                                            ('alignmentLDDT', 'REAL'),\n",
    "                                            ('fullSeqId', 'REAL')])\n",
    "    \n",
    "    # each hit is defined uniquely by the tuple of name, chain, aino acid type and position of \n",
    "    # query and hit structure\n",
    "    RESULT_STATISTICS_PRIMARY_FIELDS = (MM_QUERY_NAME, MM_QUERY_CHAIN, MM_QUERY_AA, MM_QUERY_POS,\n",
    "                                        MM_HIT_NAME, MM_HIT_CHAIN, MM_HIT_AA, MM_HIT_POS)\n",
    "    \n",
    "    # Inverse of the primary key to identify symmetric matches\n",
    "    RESULT_STATISTICS_INVERSE_KEY = [MM_HIT_NAME, MM_HIT_CHAIN, MM_HIT_AA, MM_HIT_POS, \n",
    "                                     MM_QUERY_NAME, MM_QUERY_CHAIN, MM_QUERY_AA, MM_QUERY_POS]\n",
    "    \n",
    "    \n",
    "    def __init__(self, db_type, db_name, ignore_nonstandard_aa, delimiter=','):\n",
    "        self.db_type = db_type\n",
    "        self.db_name = db_name\n",
    "        self.delimiter = delimiter\n",
    "        self.ignore_nonstandard_aa = ignore_nonstandard_aa\n",
    "        self.table_name = 'result_statistics'\n",
    "\n",
    "        if self.db_type == \"sqlite\":\n",
    "            self.conn = sqlite3.connect(self.db_name)\n",
    "            self.cur = self.conn.cursor()\n",
    "        elif self.db_type == \"postgres\":\n",
    "            raise NotImplementedError()\n",
    "#             self.conn = psycopg2.connect(database=self.db_name)\n",
    "#             self.cur = self.conn.cursor()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid database type\")\n",
    "            \n",
    "        # init table\n",
    "        self.create_resultstatistics_table()\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.cur.close()\n",
    "        self.conn.close()\n",
    "            \n",
    "    def create_resultstatistics_table(self):\n",
    "        # Create the result_statistics table\n",
    "        self.cur.execute(f'''CREATE TABLE IF NOT EXISTS {self.table_name}\n",
    "                           ({', '.join(f\"{k} {v}\" for k, v in \n",
    "                             CSVWriterToDatabase.RESULT_STATISTICS_FIELDS.items())},\n",
    "                             PRIMARY KEY ({', '.join(CSVWriterToDatabase.RESULT_STATISTICS_PRIMARY_FIELDS)})\n",
    "                           )''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def write_csv_to_database(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            reader = csv.reader(file, delimiter=self.delimiter)\n",
    "\n",
    "            header = next(reader)\n",
    "\n",
    "            col_indices_dict = OrderedDict([(col, header.index(col)) for col in CSVWriterToDatabase.RESULT_STATISTICS_FIELDS.keys()])\n",
    "\n",
    "            placeholder = '?'\n",
    "\n",
    "            # insert row into the database. While inserting we ignore duplicates (implicitly through the primary key) and \n",
    "            # ignore rows where the inverse of the row is already present in the database (the SELECT-WHERE-statement). \n",
    "            query = f\"\"\"INSERT INTO {self.table_name} ({', '.join(CSVWriterToDatabase.RESULT_STATISTICS_FIELDS.keys())}) \n",
    "                            SELECT {','.join([placeholder] * len(col_indices_dict))} WHERE NOT EXISTS (\n",
    "                            SELECT 1 FROM {self.table_name} \n",
    "                            WHERE {' AND '.join(f'{field} = {placeholder}'\n",
    "                                                for field in CSVWriterToDatabase.RESULT_STATISTICS_PRIMARY_FIELDS)})\"\"\"\n",
    "#             print(query)\n",
    "            counter_skipped_nonstandard_aa = 0\n",
    "            for row in reader:\n",
    "\n",
    "                if self.ignore_nonstandard_aa and any(row[col_indices_dict[field]] not in three_2_one_dict for field in [MM_QUERY_AA, MM_HIT_AA]):\n",
    "                    counter_skipped_nonstandard_aa += 1\n",
    "                    continue;\n",
    "\n",
    "                row_values = [row[i] for i in col_indices_dict.values()]\n",
    "                row_values.extend([row[col_indices_dict[field]] for field in CSVWriterToDatabase.RESULT_STATISTICS_INVERSE_KEY])\n",
    "#                 print(row_values)\n",
    "                self.cur.execute(query, row_values)\n",
    "    \n",
    "            print(f'skipped {counter_skipped_nonstandard_aa} non-standard AA hit rows')\n",
    "\n",
    "        self.conn.commit()\n",
    "        \n",
    "\n",
    "class DatabaseReader:\n",
    "    \"\"\"Reads from a database representing a MicroMiner results table\"\"\"\n",
    "    \n",
    "    def __init__(self, db_type, db_name):\n",
    "        self.db_type = db_type\n",
    "        self.db_name = db_name\n",
    "        self.nof_rows = None\n",
    "        self.table_name = 'result_statistics'\n",
    "\n",
    "        if self.db_type == \"sqlite\":\n",
    "            self.conn = sqlite3.connect(self.db_name)\n",
    "            self.cur = self.conn.cursor()\n",
    "        elif self.db_type == \"postgres\":\n",
    "            raise NotImplementedError()\n",
    "#             self.conn = psycopg2.connect(database=self.db_name)\n",
    "#             self.cur = self.conn.cursor()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid database type\")\n",
    "            \n",
    "    def query(self, sql_str):\n",
    "        self.cur.execute(sql_str)\n",
    "        rows = self.cur.fetchmany()\n",
    "        while len(rows) > 0:\n",
    "            yield rows\n",
    "            rows = self.cur.fetchmany()\n",
    "            \n",
    "    def get_header(self):\n",
    "        self.cur.execute(\"SELECT name FROM pragma_table_info('result_statistics') ORDER BY cid\")\n",
    "        return [elem[0] for elem in self.cur.fetchall()]\n",
    "        \n",
    "    def get_nof_rows(self):\n",
    "        if self.nof_rows is None:\n",
    "            self.nof_rows = self.cur.execute(\"SELECT COUNT(*) FROM result_statistics\").fetchone()[0]\n",
    "        return self.nof_rows\n",
    "            \n",
    "    def get_mean(self, col):\n",
    "        self.cur.execute(f\"SELECT AVG({col}) FROM {self.table_name}\")\n",
    "        avg_value = self.cur.fetchone()[0]\n",
    "        return avg_value\n",
    "    \n",
    "    def get_median(self, col):\n",
    "        self.cur.execute(f'''SELECT AVG({col})\n",
    "                             FROM (SELECT {col}\n",
    "                                   FROM {self.table_name}\n",
    "                                   ORDER BY {col}\n",
    "                                   LIMIT 2 - (SELECT COUNT(*) FROM {self.table_name}) % 2    -- odd 1, even 2\n",
    "                                   OFFSET (SELECT (COUNT(*) - 1) / 2\n",
    "                                           FROM {self.table_name}))''')\n",
    "        median_value = self.cur.fetchone()[0]\n",
    "        return median_value\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995c1ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_bidirectional_hits(reader):\n",
    "    # sanity check the database for symmetric duplicates\n",
    "    reader.cur.execute('select a.queryName,a.queryChain,a.queryAA,a.queryPos,a.hitName,a.hitChain,a.hitAA,a.hitPos,b.queryName,'\n",
    "    'b.queryChain,b.queryAA,b.queryPos,b.hitName,b.hitChain,b.hitAA,b.hitPos from result_statistics as a,result_statistics as b'\n",
    "    ' where a.queryName = b.hitName AND a.queryChain = b.hitChain AND a.queryAA = b.hitAA '\n",
    "    'AND a.queryPos = b.hitPos AND a.hitName = b.queryName AND a.hitChain = b.queryChain AND a.hitAA = b.queryAA AND a.hitPos = b.queryPos')\n",
    "    assert reader.cur.fetchone() is None\n",
    "    \n",
    "def count_non_standard_aa(reader):\n",
    "    # Count non-standard amino acid hits\n",
    "    query = f\"\"\"SELECT COUNT(*) FROM result_statistics WHERE queryAA NOT IN (\"{'\", \"'.join(three_2_one_dict.keys())}\")\n",
    "     OR hitAA NOT IN (\"{'\", \"'.join(three_2_one_dict.keys())}\")\"\"\"\n",
    "    # print(query)\n",
    "    reader.cur.execute(query)\n",
    "    return reader.cur.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b65bbf",
   "metadata": {},
   "source": [
    "### Handle monomer mode results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529835e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 350968 non-standard AA hit rows\n"
     ]
    }
   ],
   "source": [
    "writer = CSVWriterToDatabase(db_type=\"sqlite\", ignore_nonstandard_aa=True, db_name=db_name_monomer, delimiter='\\t')\n",
    "writer.write_csv_to_database(csv_monomer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38bbbc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_monomer = DatabaseReader(db_type=\"sqlite\", db_name=db_name_monomer)\n",
    "sanity_check_bidirectional_hits(reader_monomer)\n",
    "assert count_non_standard_aa(reader_monomer)[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea9d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter results down\n",
    "query = 'SELECT *, MAX(fullSeqId) from result_statistics WHERE fullSeqId > 0.4 GROUP BY queryName, queryAA, queryChain, queryPos, hitAA'\n",
    "with open('/local/sieg/filtered_single_mutations_pdb_monomer.tsv', 'w') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    csv_writer.writerow(reader_monomer.get_header())\n",
    "    for rows in reader_monomer.query(query):\n",
    "        csv_writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb29656",
   "metadata": {},
   "source": [
    "### handle ppi mode results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571e4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 114670 non-standard AA hit rows\n"
     ]
    }
   ],
   "source": [
    "writer = CSVWriterToDatabase(db_type=\"sqlite\", ignore_nonstandard_aa=True, db_name=db_name_ppi, delimiter='\\t')\n",
    "writer.write_csv_to_database(csv_ppi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea18cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_ppi = DatabaseReader(db_type=\"sqlite\", db_name=db_name_ppi)\n",
    "sanity_check_bidirectional_hits(reader_ppi)\n",
    "assert count_non_standard_aa(reader_ppi)[0] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36dee205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter results down\n",
    "query = 'SELECT *, MAX(fullSeqId) from result_statistics WHERE fullSeqId > 0.4 GROUP BY queryName, queryAA, queryChain, queryPos, hitAA'\n",
    "with open('/local/sieg/filtered_single_mutations_pdb_ppi.tsv', 'w') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    csv_writer.writerow(reader_ppi.get_header())\n",
    "    for rows in reader_ppi.query(query):\n",
    "        csv_writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c0983",
   "metadata": {},
   "source": [
    "### handle mutations to non-standard residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484d1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mutations_non_standard_aas(file_path):\n",
    "    result_rows = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        header = next(reader)\n",
    "        col_indices_dict = OrderedDict([(col, header.index(col)) for col in [MM_QUERY_AA, MM_HIT_AA]])\n",
    "        for row in reader:\n",
    "            if any(row[col_indices_dict[field]] not in three_2_one_dict for field in [MM_QUERY_AA, MM_HIT_AA]):\n",
    "                result_rows.append(row)\n",
    "    return header, result_rows\n",
    "\n",
    "with open('/local/sieg/single_mutations_pdb_monomer_non_standard_aa.tsv', 'w') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    header, rows = extract_mutations_non_standard_aas(csv_monomer)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(rows)        \n",
    "    \n",
    "with open('/local/sieg/single_mutations_pdb_ppi_non_standard_aa.tsv', 'w') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    header, rows = extract_mutations_non_standard_aas(csv_ppi)\n",
    "    csv_writer.writerow(header)\n",
    "    csv_writer.writerows(rows)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
